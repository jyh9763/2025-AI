{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005a6a98",
   "metadata": {},
   "source": [
    "# 딥러닝에서 활성화 함수란?\n",
    "딥러닝에서 **활성화 함수**는 뉴런이 얼마나 활성화(작동)될지를 결정하는 역할을 합니다. \n",
    "쉽게 말해, 입력값을 받아서 어떤 값을 출력할지 정해주는 규칙이라고 생각하면 됩니다.\n",
    "\n",
    "여기서는 세 가지 활성화 함수인 **ReLU 함수**, **Sigmoid 함수**, 그리고 **Softmax 함수**를 배워볼 거예요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bf4cd",
   "metadata": {},
   "source": [
    "## ReLU 함수란?\n",
    "ReLU(Rectified Linear Unit) 함수는 아주 간단합니다:\n",
    "- 입력값이 **0보다 작으면 0**을 출력합니다.\n",
    "- 입력값이 **0보다 크면 입력값 그대로**를 출력합니다.\n",
    "\n",
    "### ReLU 함수의 동작 예제\n",
    "- 입력값이 -3이면? → 0 (0보다 작으니까)\n",
    "- 입력값이 5이면? → 5 (0보다 크니까)\n",
    "- 입력값이 0이면? → 0 (그대로 출력)\n",
    "\n",
    "ReLU 함수는 딥러닝에서 뉴런이 활성화될지 말지를 결정하는 데 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d40cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ReLU 함수 정의\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU 함수: 입력값이 0보다 작으면 0, 0보다 크면 입력값 그대로 반환합니다.\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray or float): 입력 값\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray or float: ReLU 결과 값\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# 입력값 예제\n",
    "inputs = np.array([-3, -1, 0, 2, 5])\n",
    "\n",
    "# ReLU 함수 적용\n",
    "relu_outputs = relu(inputs)\n",
    "\n",
    "print(\"입력값:\", inputs)\n",
    "print(\"ReLU 출력값:\", relu_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d65710",
   "metadata": {},
   "source": [
    "## Sigmoid 함수란?\n",
    "Sigmoid 함수는 입력값을 **0과 1 사이의 값**으로 변환합니다. \n",
    "쉽게 말해, 입력값이 작으면 0에 가까운 값을, 크면 1에 가까운 값을 출력합니다.\n",
    "\n",
    "### Sigmoid 함수의 동작 예제\n",
    "- 입력값이 -2이면? → 0.12 (0에 가까운 값)\n",
    "- 입력값이 2이면? → 0.88 (1에 가까운 값)\n",
    "- 입력값이 0이면? → 0.5 (정확히 중간값)\n",
    "\n",
    "Sigmoid 함수는 출력값이 확률처럼 해석될 수 있어서 분류 문제에서 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c80f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid 함수 정의\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid 함수: 입력값을 0과 1 사이로 변환합니다.\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray or float): 입력 값\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray or float: Sigmoid 결과 값\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 입력값 예제\n",
    "inputs = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Sigmoid 함수 적용\n",
    "sigmoid_outputs = sigmoid(inputs)\n",
    "\n",
    "print(\"입력값:\", inputs)\n",
    "print(\"Sigmoid 출력값:\", sigmoid_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7227f06",
   "metadata": {},
   "source": [
    "## Softmax 함수란?\n",
    "Softmax 함수는 여러 개의 값을 받아서 **각 값이 차지하는 비율**을 계산합니다. \n",
    "쉽게 말해, 입력값을 확률처럼 변환해서 전체 합이 1이 되도록 만듭니다.\n",
    "\n",
    "### Softmax 함수의 동작 예제\n",
    "- 입력값이 [2.0, 1.0, 0.1]이라면?\n",
    "  - Softmax 함수는 각 값을 계산해서 [0.659, 0.242, 0.099]로 변환합니다.\n",
    "  - 이 값들의 합은 항상 1이 됩니다.\n",
    "\n",
    "Softmax 함수는 분류 문제에서 **여러 클래스 중 하나를 선택**할 때 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 함수 정의\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax 함수: 입력값을 확률 분포로 변환합니다.\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): 입력 값\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Softmax 결과 값\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # 입력값의 최대값을 빼서 안정성 확보\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# 입력값 예제\n",
    "inputs = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "# Softmax 함수 적용\n",
    "softmax_outputs = softmax(inputs)\n",
    "\n",
    "print(\"입력값:\", inputs)\n",
    "print(\"Softmax 출력값:\", softmax_outputs)\n",
    "print(\"출력값의 합:\", np.sum(softmax_outputs))  # 항상 1이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20800e",
   "metadata": {},
   "source": [
    "## ReLU, Sigmoid, Softmax 함수 비교\n",
    "- **ReLU 함수**: 0보다 작으면 0, 크면 그대로 출력합니다.\n",
    "- **Sigmoid 함수**: 입력값을 0과 1 사이로 변환합니다.\n",
    "- **Softmax 함수**: 여러 값을 받아서 확률 분포로 변환합니다 (합이 1이 됨).\n",
    "\n",
    "이 함수들은 딥러닝에서 각각 다른 상황에서 사용되며, 모델이 더 복잡한 패턴을 학습할 수 있도록 도와줍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

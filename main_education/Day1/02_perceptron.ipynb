{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 활성화 함수(Activation Function) 알아보기\n",
    "\n",
    "이 노트북은 딥러닝에서 사용되는 주요 활성화 함수들을 이해하고, Python 코드로 직접 구현하며 시각화하는 것을 목표로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 왜 활성화 함수가 필요할까?\n",
    "\n",
    "신경망의 기본 계산은 `입력(x) * 가중치(w) + 편향(b)` 입니다. 이는 선형(linear) 계산입니다. 만약 활성화 함수가 없다면, 신경망을 아무리 깊게 쌓아도 결국 하나의 큰 선형 계산에 불과합니다. \n",
    "\n",
    "**활성화 함수**는 이 선형 계산 결과에 **비선형(non-linear)** 변환을 가해, 신경망이 훨씬 더 복잡하고 다양한 패턴을 학습할 수 있도록 만들어주는 핵심적인 역할을 합니다. 마치 신호를 다음으로 보낼지 말지, 보낸다면 얼마나 강하게 보낼지를 결정하는 '스위치'나 '볼륨 조절기'와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그래프 스타일 설정\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 활성화 함수의 종류\n",
    "\n",
    "### 2-1. 계단 함수 (Step Function)\n",
    "\n",
    "가장 단순한 형태의 활성화 함수입니다. 입력값이 0을 넘으면 1을 출력하고, 그렇지 않으면 0을 출력합니다. 마치 전등 스위치처럼 'ON' 또는 'OFF'만 존재합니다. 초창기 모델인 퍼셉트론에서 사용되었지만, 지금은 거의 사용되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    # x가 0보다 크면 True(1), 작거나 같으면 False(0)가 됩니다.\n",
    "    # dtype=int를 통해 boolean 값을 정수 0 또는 1로 바꿔줍니다.\n",
    "    return np.array(x > 0, dtype=int)\n",
    "\n",
    "# -5부터 5까지 0.1 간격으로 숫자 생성\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = step_function(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Step Function')\n",
    "plt.ylim(-0.1, 1.1) # y축 범위 지정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 시그모이드 함수 (Sigmoid Function)\n",
    "\n",
    "계단 함수를 부드러운 곡선 형태로 바꾼 것입니다. 출력값이 항상 0과 1 사이의 값을 가집니다. 이 특징 때문에 어떤 사건이 일어날 '확률'을 나타낼 때 유용하게 사용됩니다. 하지만, 입력값이 너무 크거나 작아지면 그래프의 기울기가 0에 가까워져 학습이 잘 되지 않는 **'기울기 소실(Vanishing Gradient)'** 문제가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Sigmoid Function')\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 렐루 함수 (ReLU, Rectified Linear Unit)\n",
    "\n",
    "최근 가장 널리 사용되는 활성화 함수입니다. 규칙은 아주 간단합니다.\n",
    "- 입력이 0보다 크면, 입력을 그대로 출력한다.\n",
    "- 입력이 0보다 작거나 같으면, 0을 출력한다.\n",
    "\n",
    "계산이 매우 빠르고, 시그모이드 함수의 기울기 소실 문제를 해결해주기 때문에 많은 딥러닝 모델에서 좋은 성능을 냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = relu(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('ReLU Function')\n",
    "plt.ylim(-1.0, 5.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 신경망에 활성화 함수 적용하기\n",
    "\n",
    "이제 간단한 신경망 계산에 시그모이드 활성화 함수를 적용해 보겠습니다. `np.dot(x, w) + b` 로 계산된 선형 결과 `a1`을 `sigmoid()` 함수에 넣어 최종 결과 `z1`을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 신경망의 가중치와 편향 설정\n",
    "network = {}\n",
    "network['w1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "# 2. 입력값 설정\n",
    "x = np.array([1.0, 0.5])\n",
    "\n",
    "# 3. 선형 계산 (가중치 합)\n",
    "a1 = np.dot(x, network['w1']) + network['b1']\n",
    "\n",
    "print(f\"선형 계산 결과 (a1): {a1}\")\n",
    "\n",
    "# 4. 활성화 함수(시그모이드) 적용\n",
    "z1 = sigmoid(a1)\n",
    "\n",
    "print(f\"활성화 함수 적용 후 최종 결과 (z1): {z1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 함수 비교\n",
    "\n",
    "세 가지 활성화 함수를 한 번에 그려 비교해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y_step = step_function(x)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "plt.plot(x, y_step, linestyle='--', label='Step')\n",
    "plt.plot(x, y_sigmoid, label='Sigmoid')\n",
    "plt.plot(x, y_relu, linestyle='-.', label='ReLU')\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.title('Activation Functions Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
